nb_stats = length(out.flows)
# Get the poc and doc flows
poc = flows[1,'POC net']
doc = flows[1,'DOC net']
# Get the sample location meta data
cruise = flows[1,'Cruise']
station = flows[1,'Station']
station = gsub('/', '-', station)
#=========================================
# Stability of the optimisation process
#=========================================
nb.comb.params = 100
best.params.values = matrix(NA,nb.comb.params, nb_params - nb_fixed_entries + 1)
for (comb in 1:nb.comb.params){
# The initial parameters values to launch the algorithm
starting.values = c(c(poc, doc, runif(1), runif(1)/10, runif(1)/10, runif(1), runif(1)*6, runif(1)*6), out.flows[1,])
#starting.values = c(c(poc, doc, 0.75, 0.02, 0.08, 0.5, 0.44, 0.44), out.flows[1,])
starting.values = as.numeric(starting.values)
# Lower/upper bounds contraints for each parameter.
# The optimisation algorithm needs a non-null range for all params
# => Use an epsilon to have a [poc - epsilon, poc + epsilon] range (same for the doc)
epsilon = 1E-8
lb = c(c(poc - epsilon, doc - epsilon), rep(0, nb_params - nb_fixed_entries), as.numeric(out.flows[1,]) - epsilon)
lb = as.numeric(lb)
ub = c(c(poc + epsilon, doc + epsilon, 1, 0.1, 0.1, 1, 6, 6), as.numeric(out.flows[1,]) + epsilon) # Lower bounds contraints for each parameter
ub = as.numeric(ub)
# Performs a gradient-free optimisation
# The goal is to minimize the distance existing between the measured
# outflows and their model counterparts
start = Sys.time()
res.station = nmkb(par = starting.values, fn = pred.error, lower=lb, upper=ub)
end  = Sys.time()
print(end - start)
print(res.station$message)
# Store the values of the parameters and the associated loss
best.comb = res.station$par[(nb_fixed_entries + 1):nb_params]
best.comb = c(best.comb, res.station$value)
best.params.values[comb,] = best.comb
}
res = data.frame(best.params.values)
colnames(res) = c(labels, 'loss')
#===========================================
# Find the best combinaison and compute CIs
#===========================================
# Estimation
min.loss = min(res$loss)
best.comb = res[res$loss == min.loss,]
# CI bounds
q05.comb = apply(res, 2, quantile, 0.05) # Lower bound
q95.comb = apply(res, 2, quantile, 0.95) # Upper bound
# Store the results
estim = as.data.frame(t(rbind(best.comb, q025.comb, q975.comb)))
colnames(estim) = c('estimation', 'lb', 'ub')
estim$estimation = formatC(estim$estimation, format = "e", digits = 1)
estim$lb = formatC(estim$lb, format = "e", digits = 1)
estim$ub = formatC(estim$ub, format = "e", digits = 1)
estim$CI = str_c('(', str_c(estim[,c('lb')],
' ; ', estim[, c('ub')] ), ')')
estim = subset(estim, select = -c(lb, ub) )
#===========================================
# Store and plot the results
#===========================================
estim.csv <- tibble::rownames_to_column(estim, "variable")
write.table(estim.csv,
col.names = T,
file.path(res.folder, 'params',
paste(cruise, station, ".csv")),
row.names = F, sep = ',')
## Add units ?
#*******************
# Psi and alpha
#*******************
psi_alpha = res %>%
pivot_longer(c(psi, alpha),
names_to = "variable", values_to = "value")
psi_alpha.bp <- ggplot(psi_alpha, aes(x=variable, y=value)) +
labs(y="value", x = "") +
geom_boxplot()
#*******************
# The CFs
#*******************
CFs = res %>%
pivot_longer(c(CF_A, CF_FL),
names_to = "variable", values_to = "value")
CFs.bp <- ggplot(CFs, aes(x=variable, y=value)) +
labs(y="value", x = "CF") +
geom_boxplot()
#*******************
# The PGEs
#*******************
PGEs = res %>%
pivot_longer(c(wA, wFL),
names_to = "variable", values_to = "value")
PGEs.bp <- ggplot(PGEs, aes(x=variable, y=value)) +
labs(y="value", x = "PGE") +
geom_boxplot()
#*******************
# The loss
#*******************
loss = res %>%
pivot_longer(c(loss),
names_to = "variable", values_to = "value")
loss.bp <- ggplot(loss, aes(x=variable, y=value)) +
labs(y="value", x = "") +
geom_boxplot()
#*******************
# All plots
#*******************
png(filename = file.path(res.folder, 'estim.boxplots.png'))
plot_grid(psi_alpha.bp,
CFs.bp,
PGEs.bp,
loss.bp,
labels = c('', '', ''),
label_size = 12)
dev.off()
##################################################
# Exercice 1
##################################################
# ACP sur les données IRIS
# EXERCICE 1
library(factoextra)
library(FactoMineR)
# Donnees IRIS
X = iris
Xnum = X[,1:4]
N = nrow(Xnum)
# K est le nombre de modalités d'iris: 3 dans notre cas.
K = length(levels(X$Species))
# On compte le nombre d'individus appartenant à chacune des modalités.
n1 = nrow(X[X$Species=='setosa',])
n2 = nrow(X[X$Species=='versicolor',])
n3 = nrow(X[X$Species=='virginica',])
# On réalise l'ACP sur les données iris
resPCA = prcomp(Xnum, scale = TRUE) # Pas besoin de scaler avant, la fonction
# le fait en interne.
# Visualisation automatique
fviz_eig(resPCA) # Les valeurs propres décroissent rapidement:
# On peut résumer beaucoup d'informations avec les toutes premières valeurs propres.
# La première dimension résume plus de 70% de l'information, plus de 20% pour la deuxième.
# Représentation des variables
fviz_pca_var(resPCA) # La dimension 1 résume bien la longueur et la largeur des pétales et la longueur des sépales.
# Ces variables contiennent beaucoup d'information redondante.
# La dimension 2 est négativement corrélée avec la largeur des sépales.
fviz_pca_ind(resPCA, label="none", habillage=iris$Species)
# Les trois groupes apparaissent bien séparés.
# Ils sont donc bien reconnaissables à l'aide des nouvelles composantes.
plot(resPCA$x[,1], col =  iris$Species)
##################################################
# Exercice 2
##################################################
# AFD sur les données IRIS
Z = scale(Xnum) # donnees centrees reduites
#Q1: covariance des données centrées-réduites = corrélation des données
R = cov(Z)
#Q2 centres de gravite
Z1 = Z[X$Species=='setosa',]
Z2 = Z[X$Species=='versicolor',]
Z3 = Z[X$Species=='virginica',]
g1 = apply(Z1, 2, mean)
g2 = apply(Z2, 2, mean)
g3 = apply(Z3, 2, mean)
G = rbind(g1,g2,g3) # On "colle ensemble" les 3 vecteurs
# Q3 variance: intra classe
S1 = cov(Z1)
S2 = cov(Z2)
S3 = cov(Z3)
S = (1/N)*(n1*S1 + n2*S2 + n3*S3) # Moyenne pondérée des variances intra
# Q4 variance inter classe
P = diag(c(n1,n2,n3),3,3) # P contient les effectifs de chaque classe sur la
# diagonale
T = (1/N)*(t(G) %*% P %*% G)
# Q5
delta = abs(R-S-T)
print(delta) # L'erreur est faible. On a donc bien: R = S + T
#Q6 diagonalisation
# On cherche ici à maximiser le pourcentage d'information discriminante
# C'est-à-dire le rapport variance inter/variance total: R^{-1}T
# On calcule donc les valeurs/vecteurs propres de cette matrice
inv.R = solve(R)
Mat = inv.R %*% T
diag = eigen(Mat)
#Q7 variables discriminantes
A = diag$vectors[,1:2]
U = Z %*% A
#Q8 representation des individus
par(pty="m")
plot(U, col=X$Species, pch = 16, xlab="Composante discriminante 1", ylab="Composante discriminante 2")
legend('topright', legend = levels(X$Species), col = 1:3, cex = 0.8, pch = 16)
#Q9: AFD
library(MASS)
iris_scale = X
iris_scale[,1:4] = Z
resAFD = lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris_scale)
plot(resAFD) # Assez sale
# On finit à la main
U2 = Z %*% resAFD$scaling
U2[,1] = U2[,1] # (paranoia) On s'assure que la matrice soit parfaitement symmétrique
# Il existe peut-être une façon plus simple de faire le plot de l'AFD
par(pty="m")
plot(U2, col=X$Species, pch = 16)
legend('topright', legend = levels(X$Species), col = 1:3, cex = 0.8, pch = 16)
library(factoextra)
folder = 'C:/Users/rfuchs/Documents/These/Enseignement/Stats M2 IEMH/2020-2021/TPs/TP3'
X = read.csv(file.path(folder, 'pima_indians.csv'))
Xnum = X[,1:(ncol(X) -1)] # Selectionne pas la derniere colonne qui contient les classes
Xnum = X[,1:(ncol(X) -1)] # Selectionne pas la derniere colonne qui contient les classes
Xnum
Y = X[,ncol(X)]
Y
N = length(Y)
N
K = length(levels(factor(Y)))
K
n1 = length(Y[Y==0])
n2 = length(Y[Y==1])
n1
n2
# ACP sur les iris
resPCA = prcomp(Xnum, scale = TRUE)
fviz_eig(resPCA) # Les valeurs dÃ©croissent assez lentement
fviz_pca_var(resPCA) # Visualiser les variables
fviz_pca_ind(resPCA, label="none", habillage=Y) # Visualiser les individus
############################################
# EXERCICE 2
############################################
library(MASS)
Z = scale(Xnum)
resAFD = lda(Y ~ Z)
plot(resAFD)
U2 = Z %*% resAFD$scaling # %*% Produit matriciel
U2
par(pty="m")
plot(U2, col=factor(Y), pch = 16)
legend('topright', legend = levels(factor(Y)), col = 1:3, cex = 0.8, pch = 16)
# Train test split
set.seed(123) # set the seed to make your partition reproducible
n = nrow(Xnum)
# Train test split
set.seed(123) # set the seed to make your partition reproducible
n = nrow(Xnum)
n
smp_size <- floor(0.75 * n)
smp_size
0.75 * n
seq_len(n)
smp_size
train_ind <- sample(seq_len(n), size = smp_size)
train_ind
X_train <- Z[train_ind, ]
X_test <- Z[-train_ind, ]
Y_train = Y[train_ind]
Y_test = Y[-train_ind]
# Train test split
set.seed("123") # set the seed to make your partition reproducible
train = data.frame(cbind(X_train, Y_train))
test = data.frame(cbind(X_test, Y_test))
resAFD = lda(Y_train ~ ., data = train)
resAFD
preds = predict(resAFD, newdata = test)$class
preds
accuracy = mean(preds == Y_test)
accuracy
windows() ## create window to plot your file
partimat(as.factor(Y_test) ~ .,data=test,method="lda")
#dev.off()
windows() ## create window to plot your file
partimat(as.factor(Y_test) ~ .,data=test,method="lda")
dev.off()
windows() ## create window to plot your file
partimat(as.factor(Y_test) ~ .,data=test,method="lda")
dev.off()
remove.packages("ssPopModel")
library(devtools)
library(ssPopModel)
library(plotly)
library(tidyverse)
library("lattice")
library('gridExtra')
#======================================
# Settings
#======================================
path = "C:/Users/rfuchs/Documents/These/Oceano/Upwellings/datasets/INSITU/final/biovolume_classes"
res.folder = '../../../../results'
setwd(path)
colors = hcl.colors(30, "Zissou 1")
# Need for the 24 hours of the day and the first hour
# of the next day to estimate 24 hourly growth rates
nb.hours = 24
E.points.per.hour = 6
# Phytoplankton
pfg = 'ORGNANO'
## Biomass conversion coefficients
if (pfg %in% c('ORGPICOPRO', 'REDPICOPRO', 'REDPICOEUK')) {
a = 0.26
b = 0.86
} else {
a = 0.433
b = 0.863
}
#=======================================
# Load PAR and events data
#=======================================
# Load PAR data separately (not the same sampling frequency)
E <- read_csv(paste0('../../../par/final/par_10min.csv'))
colnames(E) = c('time','par')
E.days = format(strptime(E$time,'%Y-%m-%d %H:%M:%S'),'%Y-%m-%d')
#=======================================
# Load PAR and events data
#=======================================
events = read.csv('../../../../results/event_T_anomalies_suited.csv')
events = events[,c('window_start', 'window_end')]
n.events = dim(events)[1]
#=======================================
# Iterate through the events
#=======================================
# Create the results storage
all.dates.params = c()
NPP.size.ts = c()
mu.ts = c()
loss.ts = c()
start <- Sys.time()
for (event.idx in 1:n.events){
# Event metadata
event = events[event.idx,]
days = seq(as.Date(event$window_start), as.Date(event$window_end), by="days")
n.days = length(days)
# Import phytoplankton data
if (file.exists(paste0(pfg, '/', as.character(days[1]), '.csv'))){
df <- read_csv(paste0(pfg, '/', as.character(days[1]), '.csv'))
}else{ # If no corresponding phytoplankton data, skip the event
print(paste0('event', as.character(days[1]),' has no phytoplankton data'))
next
}
# Matching the new data format:
#colnames(df)[1] <- c("date")
# Format the phytoplankton data
dist <- add_column(df, pop = pfg, .after = "time")
dist <- add_column(dist, par = NA, .after = 'pop')
dfh <- transform_PSD(dist, time.step="1 hour", interval.to.geomean=T)
# Get all the dates of the series
data.days = format(strptime(dfh$time,'%Y-%m-%d %H:%M:%S'),'%Y-%m-%d')
E.days = format(strptime(E$time,'%Y-%m-%d %H:%M:%S'),'%Y-%m-%d')
# Estimate per day growth rate
for (day.idx in 1:n.days){
print(day.idx)
# Phyto data
distribution.day <- dfh[data.days == days[day.idx],-c(2, 3)]
distribution.next.day <- dfh[data.days == days[day.idx + 1],-c(2, 3)]
distribution <- bind_rows(distribution.day, distribution.next.day[c(1),])
if (length(distribution$time) != 25){
print(length(distribution$time))
print('Lacks data for that day')
next
}
# Par data
E.data = E[E.days == days[day.idx],]
# TO DO: Change the break in continue once has shown to be effective
if (sum(is.na(E.data$par)) > 0){
print('NaNs in E')
next
}
if (sum(is.na(distribution)) > 0){
print('Missing distrib values')
next
}
if (nrow(E.data) == 0){
print('No PAR DATA')
next
}
#=================================
# Smooth the distributions
#=================================
# Smooth the size distribution
dist <- round(t(apply(distribution[,-c(1)],1, function(x) smooth.spline(x, all.knots=TRUE)$y)))
dist[which(dist < 1)] <- 0
distribution[,-c(1)] <- dist
ma <- round(apply(distribution[,-c(1, 2)], 1, function(x) sum(x,na.rm=T)))
Vdist <- as.matrix(distribution[,-c(1, 2)]/ma)
plot_ly(z= Vdist) %>%
add_surface() %>%
layout(scene = list(xaxis = list(autorange = "reversed")))
# No need for PAR smoothing
#=================================
# Model estimation
#=================================
volbins <- as.numeric(colnames(distribution)[-c(1)])
resol <- 10
dt <- resol/60
output <- ssPopModel::determine_opt_para(distribution,E.data$par,resol)
#=================================
# Format the output
#=================================
#*************************
#! Growth rate TO CHECK
#*************************
params <- output$parameters
PSD <- output$PSD
mu_N <- diff(log(rowSums(PSD[,-c(1)], na.rm=T))) / as.numeric(diff(PSD$time))
d.mu_N <- sum(mu_N, na.rm = T)
mu_N.obs = diff(log(rowSums(distribution[,-c(1)], na.rm=T))) / as.numeric(diff(distribution$time))
d.mu_N.obs <- sum(mu_N.obs, na.rm=T)
loss_N = mu_N - mu_N.obs
#*************************
# Frequency distribution
#*************************
s <- round(apply(distribution[,-c(1)], 1, function(x) sum(x,na.rm=T)))
Nproj <- as.matrix(PSD[,-c(1)])
s2 <- round(apply(Nproj, 1, function(x) sum(x,na.rm=T)))
Vproj <- as.matrix(Nproj/s2)
#print(d.mu_N)
#print(paste("daily growth rate=",round(d.mu_N,2)))
#*************************
# Primary production
#*************************
N.diff = apply(PSD[,-c(1)], 2, diff) # Compute hourly difference per class
class.C = a * as.numeric(colnames(N.diff))^b # Carbon per class conversion
cells.C = N.diff %*% diag(class.C) # Carbon per cell conversion
cells.C = cells.C * 1e-09 # From pgC to mgC
NPP.size = rowSums(cells.C)
#=================================
# Save the results of the day
#=================================
#****************************
# Observed vs predicted plot
#****************************
hours = format(strptime(distribution$time,'%Y-%m-%d %H:%M:%S'),'%H')
hours = as.numeric(hours)
day = format(strptime(distribution$time,'%Y-%m-%d %H:%M:%S'),'%Y-%m-%d')
d.data = distribution[1:nb.hours,-c(1)]/s
m = dim(d.data)[2]
d.data$hour = hours[1:nb.hours]
d.data = d.data %>% gather(SizeClass, freq,
colnames(d.data)[1]:colnames(d.data)[m])
d.data$SizeClass = as.numeric(d.data$SizeClass)
plot1 <- levelplot(freq ~ hour*SizeClass, data=d.data ,
main="Observed", col.regions = colors)
d.pred = as.data.frame(Vproj[1:nb.hours,])
d.pred$hour = hours[1:nb.hours]
d.pred = d.pred %>% gather(SizeClass, freq,
colnames(d.pred)[1]:colnames(d.pred)[m])
d.pred$SizeClass = as.numeric(d.pred$SizeClass)
plot2 <- levelplot(freq ~ hour*SizeClass, data=d.pred,
main="Predicted", col.regions = colors)
jpeg(file = file.path(res.folder, 'growth_rates', 'predictions',pfg,
paste0(day[1],".jpg")))
plot = grid.arrange(plot1,plot2, ncol=2, top = unique(day)[1])
dev.off()
#****************************
# Parameters
#****************************
params = cbind(day[1], params)
all.dates.params = rbind(all.dates.params, params)
# Store them at each iteration (if a run fails)
write.table(all.dates.params,
col.names = T,
file.path(res.folder, 'growth_rates', 'best_params',
paste0(pfg,".csv")),
sep = ',',
row.names = F)
#****************************
# NPP.size
#****************************
NPP.size = cbind(distribution$time[1:nb.hours], NPP.size)
colnames(NPP.size) = c('date', 'NPP.size')
NPP.size.ts = rbind(NPP.size.ts, NPP.size)
# Store them at each iteration (if a run fails)
write.table(NPP.size.ts,
col.names = T,
file.path(res.folder, 'growth_rates', 'NPP',
paste0(pfg, ".csv")),
sep = ',',
row.names = F)
#****************************
# Division series
#****************************
# Hack here on the nb of values of mu_N
mu.day = data.frame(distribution$time[1:nb.hours], mu_N)# Legacy: [1:nb.hours]
colnames(mu.day) = c('date', 'mu')
mu.ts = rbind(mu.ts, mu.day)
# Store them at each iteration (if a run fails)
write.table(mu.ts,
col.names = T,
file.path(res.folder, 'growth_rates','mu',
paste0(pfg, ".csv")),
sep = ',',
row.names = F)
#****************************
# Division series
#****************************
# Hack here on the nb of values of mu_N
loss.day = data.frame(distribution$time[1:nb.hours], loss_N)# [1:nb.hours]
colnames(loss.day) = c('date', 'loss')
loss.ts = rbind(loss.ts, loss.day)
# Store them at each iteration (if a run fails)
write.table(loss.ts,
col.names = T,
file.path(res.folder, "growth_rates", 'loss',
paste0(pfg, ".csv")),
sep = ',',
row.names = F)
#****************************
# Division plot
#****************************
jpeg(file = file.path(res.folder, 'growth_rates', 'cell_cycles', pfg,
paste0(day[1],".jpg")))
plot(mu_N, frame = FALSE, main = day[1], ylab="Div rate (h-1)", xlab='time (hours)')
dev.off()
#day.idx = day.idx + 1
}
}
end <- Sys.time()
